% harper

\section{Syntax}

\subsection{Introduction}


We will follow the structure of syntax outlined in Harper \cite{harper_2016}. There are several reasons for this. 

Firstly, for example in Barendregt et. al. \cite{BarendregtHenk2013Lcwt} we have notions of substitution left to the reader under the assumption that they can be fixed. Generally Barendregt's style is like this and even when there is much formalism, it is done in a way that we find peculiar.

In Crole's book \cite{CroleRoyL1993Cft}, syntax is derived from an \textit{algebraic signature} which comes directly from categorical semantics. We want to give an independent view of type theory. The syntax only has types as well, meaning that only terms can be posed in this syntax. Operations on types themselves would have to be handled separately. This will also make it difficult to work with \textit{bound variables}.

In Lambek and Scott's book \cite{LambekJ1986Itho}, very little attention is given to syntax and categorical semantics and deriving type theory from categories for study is in the forefront of their focus.

In Jacob's book \cite{JacobsCLTT}, we again have much reliance on categorical machinery. A variant of algebraic signature called a \mathit{many-typed signature} is given, which has its roots in mathematical logic. Here it is discussed that classically in logic the idea of a sort and a type were synonymous, and they go onto preferring to call them types. This still has the problems identified before as terms and types being treated separately, when it comes to syntax.

In Barendregt's older book \cite{barendregt1984lambda}, there are models of the syntax of (untyped) lambda calculus, using Scott topologies on complete lattices. We acknowledge that this is a working model of the lambda calculus but we believe it to be overly complex for the task at hand. It introduces a lot of mostly irrelevant mathematics for studying the lambda calculus. And we doubt very much that these models will hold up to much modification of the calculus. Typing seems impossible.

In S{\o}rensen and Urzyczyn's book \cite{Sorensen:2006:LCI:1197021} a more classical unstructured approach to syntax is taken. This is very similar to the approaches that Church, Curry and de Brujin gave early on. The difficulty with this approach is that it is very hard to prove things about the syntax. There are many exceptional cases to be weary of (for example if a variable is bound etc.). It can also mean that the syntax is vulnerable to mistakes. We acknowledge it's correctness in this case, however we prefer to use a safer approach.

We will finally look at one more point of view, that of mathematical logic. We look at Troelstra and Schwichtenberg's book \cite{troelstra_schwichtenberg_2000} which studies proof theory. This is essentially the previous style but done to a greater extent, for they use that kind of handling of syntax to argue about more general logics. As before, we do not choose this approach.

We have seen books from either end of the spectrum, on one hand Barendregt's type theoretic camp, and on the other, the more categorical logically oriented camp. We have argued that the categorical logically oriented texts do not do a good job of explaining and defining syntax, their only interest is in their categories. The type theoretic texts also seem to be on mathematically shaky ground, sometimes much is left to the reader and finer details are overlooked.

Harper's seems more sturdy and correct in our opinion. Harper doesn't concern himself with abstraction for the sake of abstraction but rather when it will benefit the way of thinking about something. The framework for working with syntax also seems ideal to work with, when it comes to adding features to a theory (be it a type theory or otherwise).

\subsection{Abstract Syntax Trees}

We begin by outlining what exactly syntax is, and how to work with it. This will be important later on if we want to prove things about our syntax as we will essentially have good data structures to work with.

%We will begin with the notion of an {\it abstract syntax tree}. Which can be what is informally known as syntax, thus formal statements about the syntax are referring to its manifestation as an abstract syntax tree.

%% Sort
\begin{defin}[Sorts]
    Let $\mathcal{S}$ be a finite set, which we will call \textbf{sorts}. An element of $\mathcal{S}$ is called a \textbf {sort}.
\end{defin}

A sort could be a term, a type, a kind or even an expression. It should be thought of an abstract notion of the kind of syntactic element we have. Examples will follow making this clear.

%% Arity
\begin{defin}[Arities]
    An \textbf{arity} is an element $((s_1,\dots,s_n),s)$ of the set of \textbf{arities} $\mathcal{Q}:=\mathcal{S}^\star \times \mathcal{S}$ where $\mathcal{S}^\star$ is the Kleene-star operation on the set $\mathcal{S}$ (a.k.a the free monoid on $\mathcal{S}$ or set of finite tuples of elements of $\mathcal{S}$). An arity is typically written as $(s_1,\dots,s_n)s$. 
\end{defin}

%% Operator
\begin{defin}[Operators]
    Let $\mathcal{O} :=\{ \mathcal{O}_\alpha \}_{\alpha \in \mathcal{Q}}$ be an $\mathcal{Q}$-indexed (arity-indexed) family of disjoint sets of \textbf{operators} for each arity. An element $o \in \mathcal{O}_\alpha$ is called an \textbf{operator} of arity $\alpha$. If $o$ is an operator of arity $(s_1,\dots,s_n)s$ then we say $o$ has \textbf{sort} $s$ and that $o$ has $n$ \textbf{arguments} of sorts $s_1,\dots,s_n$ respectively.
\end{defin}

%% Variables
\begin{defin}[Variables]
    Let $\mathcal{X}:= \{ \mathcal{X}_s\}_{s \in \mathcal{S}}$ be an $\mathcal{S}$-indexed (sort-indexed) family of disjoint (finite?) sets $\mathcal{X}_s$ of \textbf{variables} of sort $s$. An element $x\in\mathcal{X}_s$ is called a \textbf{variable} $x$ of \textbf{sort} $s$. 
\end{defin}

%% Fresh variables
\begin{defin}[Fresh variables]
    We say that $x$ is \textbf{fresh} for $\mathcal{X}$ if $x \not\in \mathcal{X}_s$ for any sort $s\in \mathcal{S}$. Given an $x$ and a sort $s\in \mathcal{S}$ we can form the family $\mathcal{X},x$ of variables by adding $x$ to $\mathcal{X}_s$. 
\end{defin}

%% Remark about notation for adding variables
\begin{remark}
    The notation $\mathcal{X},x$ is ambiguous because the sort $s$ associated to $x$ is not written. But this can be remedied by being clear from the context what the sort of $x$ should be.
\end{remark}

%% Abstract syntax trees
\begin{defin}[Abstract syntax trees]
    The family $\mathcal{A}[\mathcal{X}]=\{ \mathcal{A}[\mathcal{X}]_s \}_{s \in \mathcal{S}}$ of \textbf{abstract syntax trees} (or asts), of \textbf{sort} $s$, is the smallest family satisfying the following properties:
    
    \begin{enumerate}
        \item A variable $x$ of sort $s$ is an ast of sort $s$: if $x \in \mathcal{X}_s$, then $x \in \mathcal{A}[\mathcal{X}]_s$.
        
        \item Operators combine asts: If $o$ is an operator of arity $(s_1, \dots, s_n)s$, and if $a_1 \in \mathcal{A}[\mathcal{X}]_{s_1}, \dots, a_n \in \mathcal{A}[\mathcal{X}]_{s_n}$, then $o(a_1;\dots; a_n) \in \mathcal{A}[\mathcal{X}]_s$.
    \end{enumerate}
\end{defin}

[Draw a picture here]

%% Remark about inductively generated sets
\begin{remark}
    The idea of a smallest family satisfying certain properties is that of structural induction. So another way to say this would be a family of sets inductively generated by the following constructors.
\end{remark}

%% Remark about asts being trees
\begin{remark}
    An ast can be thought of as a tree whose leaf nodes are variables and branch nodes are operators. 
\end{remark}

%% Remark about proving things by structural induction on asts
\begin{remark}
    When we prove properties $\mathcal{P}(a)$ of an ast $a$ we can do so by structural induction on the cases above.
\end{remark}

[Some more notes on structural induction, perhaps this can be defined and discussed with trees in the section before?]

%% Plenty of examples of asts with examples of sorts, operators and variables
[add examples of sorts, operators, variables and how they fit together in asts]

%% Lemma about subsets of variables
\begin{lemma}
    If we have $\mathcal{X} \subseteq \mathcal{Y}$ then, $\mathcal{A}[\mathcal{X}] \subseteq \mathcal{A}[\mathcal{Y}]$.
\end{lemma}
\begin{proof}
    Suppose $\mathcal{X} \subseteq \mathcal{Y}$ and $a \in \mathcal{A}[\mathcal{X}]$, now by structural induction on $a$:
    
    \begin{enumerate}
        \item If $a$ is in $\mathcal{X}$ then it is obviously also in $\mathcal{Y}$.
        \item If $a := o(a_1;\dots;a_n) \in \mathcal{A}[\mathcal{X}]$ we have $a_1, \dots, a_n\in \mathcal{A}[\mathcal{X}]$ also. By induction we can assume these to be in $\mathcal{A}[\mathcal{Y}]$ hence giving us $a \in \mathcal{A}[\mathcal{Y}]$.
    \end{enumerate}
    
    Hence by induction we have shown that $\mathcal{A}[\mathcal{X}] \subseteq \mathcal{A}[\mathcal{Y}]$.
\end{proof}

%% Substitution
\begin{defin}[Substitution]
    If $a \in \mathcal{A}[\mathcal{X},x]_{s'}$, and $b \in \mathcal{A}[\mathcal{X}]_s$, then $[b/x]a \in \mathcal{A}[\mathcal{X}]_{s'}$ is the result of \textbf{substituting} $b$ for every occurrence of $x$ in $a$. The ast $a$ is called the \textbf{target}, the variable $x$ is called the \textbf{subject} of the \textbf{substitution}. We define substitution on an ast $a$ by induction:
    \begin{enumerate}
        \item $[b/x]x = b$ and $[b/x]y = y$ if $x\ne y$.
        \item $[b/x]o(a_1;\dots;a_n)=o([b/x]a_1;\dots;[b/x]a_n)$
    \end{enumerate}
\end{defin}

%% Examples of substitution
[Examples of substitution]

\begin{theorem}
    If $a \in \mathcal{A}[\mathcal{X},x]$, then for every $b \in \mathcal{A}[\mathcal{X}]$ there exists a unique $c \in \mathcal{A}[\mathcal{X}]$ such that $[b/x]a = c$.
\end{theorem}
\begin{proof}
    By structural induction on $a$, we have three cases: $a := x$, $a:=y$ where $y \ne x$ and $a := o(a_1; \dots; a_n)$. In the first we have $[b/x]x=b=c$ by definition. In the second we have $[b/x]y=y=c$ by definition. In both cases $c \in \mathcal{A}[\mathcal{X}]$ and are uniquely determined. Finally, when $a := o(a_1; \dots; a_n)$, we have by induction unique $c_1,\dots, c_n$ such that $c_i:=[b/x]a_i$ for $1 \le i \le n$. Hence we have a unique $c=o(c_1,\dots,c_n) \in \mathcal{A}[\mathcal{X}]$.

\end{proof}








