\section{Judgements}

\subsection{Introduction}

We will now develop the basic formal tools to describe how our programming languages work.  We will first describe judgements and how to specify a type system. Our leading example will be the simply typed lambda calculus. We use the ideas developed in \cite{harper_2016} as a basic guide line. 

\begin{defin}
    The notion of a \emph{judgement} or \emph{assertion} is a logical statement about an abt. The property or relation itself is called a \emph{judgement form}. The judgement that an object or objects have that property or stand in relation is said to be an \emph{instance} of that judgement form. A judgement form has also historically been called a \emph{predicate} and its instances called \emph{subjects}.
\end{defin}

\begin{remark}
    Typically a judgement is denoted $\mathsf{J}$. We can write $a\ \mathsf{J}$, $\mathsf{J}\ a$ to denote the judgement asserting that the judgement form $\mathsf{J}$ holds for the abt $a$. For more abts this can also be written prefix, infix, etc. This will be done for readability. Typically for an unspecified judgement, that is an instance of some judgement form, we will write $J$.
\end{remark}

\begin{defin}
    An \emph{inductive definition} of a judgement form consists of a collection of rules of the form
    
    $$\frac
        {J_1 \quad \cdots \quad J_k}
        {J}
    $$
    
    in which $J$ and $J_1, \dots , J_k$ are all judgements of the form being defined. The judgements above the horizontal line are called the \emph{premises} of the rules, and the judgement below the line is called its \emph{conclusion}. A rule with no premises is called an \emph{axiom}.
\end{defin}

\subsection{Inference rules}

\begin{remark}
    An inference rule is read as starting that the premises are \emph{sufficient} for the conclusion: to show $J$, it is enough to show each of $J_1, \dots J_k$. Axioms hold unconditionally. If the conclusion of a rule holds it is not necessarily the case that the premises held, in that the conclusion could have been derived by another rule.
\end{remark}

\begin{example}
    Consider the following judgement from $-\ \mathsf{nat}$, where $a\ \mathsf{nat}$ is read as ``$a$ is a natural number''. The following rules form an inductive definition of the judgement form $-\ \mathsf{nat}$:

    $$\frac
        {}
        {\texttt{zero}\ \mathsf{nat}}
      \qquad\qquad\qquad
      \frac
        {a\ \mathsf{nat}}
        {\texttt{succ}(a)\ \mathsf{nat}}
    $$

    We can see that an abt $a$ is zero or is of the form $\texttt{succ}(a)$. We see this by induction on the abt, the set of such abts has an operator $\texttt{succ}$. Taking these rules to be exhaustive, it follows that $\texttt{succ}(a)$ is a natural number if and only if $a$ is.
\end{example}

\begin{remark}
    We used the word \emph{exhaustive} without really defining it. By this we mean necessary and sufficient. Which we will define now.
\end{remark}

\begin{defin}
    A collection of rules is considered to define the \emph{strongest} judgement form that \emph{closed under} (or \emph{respects}) those rules. To be closed under the rules means that the rules are \emph{sufficient} to show the validity of a judgement: $J$ holds if there is a way to obtain it using the given rules. To be the \emph{strongest} judgement form closed under the rules means that the rules are also \emph{necessary}: $J$ holds \emph{only if} there is a way to obtain it by applying the rules.
\end{defin}

Let's add some more rules to our example, to get a richer structure.

\begin{example}
    The judgement form $a = b$ expresses the equality of two abts $a$ and $b$. We define it inductively on our abts as we did for $\mathsf{nat}$.
    
    $$\frac
        {}
        {\texttt{zero} = \texttt{zero}}
      \qquad\qquad\qquad
    \frac
        {a = b}
        {\texttt{succ}(a) = \texttt{succ}(b)}
    $$
    Our first rule is an axiom declaring that \texttt{zero} is equal to itself, and our second rule shows that abts of the form $\texttt{succ}$ are equal only if their arguments are. Observe that these are exhaustive rules in that they are necessary and sufficient for the formation of $=$.
\end{example}

\subsection{Derivations}

To show that an inductively defined judgement holds, we need to exhibit a \emph{derivation} of it.

\begin{defin}
    A \emph{derivation} of a judgement is a finite composition of rules, starting with axioms and ending with the judgement. It is a tree in which each node is a rule and whose children are derivations of its premises. We sometimes say that a derivation of $J$ is evidence for the validity of an inductively defined judgement $J$.

    Suppose we have a judgement $J$ and
    $$\frac
        {J_1\quad \cdots\quad J_k}
        {J}
    $$
    is an inference rule. Suppose $\triangledown_1, \dots, \triangledown_k$ are derivations of its premises, then
    $$\frac
        {\triangledown_1\quad \cdots\quad \triangledown_k}
        {J}
    $$
    is a derivation of its conclusion. Notice that if $k=0$ then the node has no children.
\end{defin}

Writing derivations as trees can be very enlightening to how the rules compose. Going back to our example with $\mathsf{nat}$ we can give an example of a derivation.

\begin{example}
    Here is a derivation of $\texttt{succ}(\texttt{succ}(\texttt{succ}(\texttt{zero})))\ \mathsf{nat}$:
    
    \begin{prooftree}
        \AxiomC{}
        \UnaryInfC{ $\texttt{zero}\ \mathsf{nat}$ }
        \UnaryInfC{ $\texttt{succ}(\texttt{zero})\ \mathsf{nat}$ }
        \UnaryInfC{ $\texttt{succ}(\texttt{succ}(\texttt{zero}))\ \mathsf{nat}$ }
        \UnaryInfC{ $\texttt{succ}(\texttt{succ}(\texttt{succ}(\texttt{zero})))\ \mathsf{nat}$ }
    \end{prooftree}
\end{example}

\begin{remark}
    To show that a judgement is \emph{derivable} we need only give a derivation for it. There are two main methods for finding derivations:
    \begin{itemize}
        \item \emph{Forward chaining} or \emph{bottom-up construction}
        \item \emph{Backward chaining} or \emph{top-down construction}
    \end{itemize}
    
    Forward chaining starts with the axioms and works forward towards the desired conclusion. Backward chaining starts with the desired conclusion and works backwards towards the axioms.
\end{remark}

It is easy to observe the \emph{algorithmic} nature of these two processes. In fact this is an important point to think about, since it may become relevant in the future.

\begin{lemma}
    Given a derivable judgement $J$, there is an algorithm giving a derivation for $J$ by forward chaining.
\end{lemma}

\begin{proof}
    This is not a difficult algorithm to describe. We start with a set of rules $\mathcal{R} := \varnothing $ which we initially set to be empty. Now we consider all the rules that have premises in $\mathcal{R}$, initially this will be all the axioms. We add these rules to $\mathcal{R}$ and repeat this process until $J$ appears as a conclusion of one of the rules in $\mathcal{R}$. It is not difficult to see that this will necessarily give all derivations of all derivable judgements and since $J$ is derivable, it will eventually give a derivation for $J$.
\end{proof}

\begin{remark}
    Notice how we had to specify that our judgement is derivable. Since if were not, then our process would not terminate, hence would not be an algorithm. It is also worth noting that this algorithm is very inefficient since the size of $\mathcal{R}$ will grow rapidly, especially when we have more rules available. This is sort of a brute force approach. What we will need is more clever picking of the rules we wish to add. Mathematically this is an algorithm, but not in any practical sense.
\end{remark}

Forward chaining does not take into account any of the information given by the judgement $J$. The algorithm is in a sense blind. 

\begin{lemma}
    Given a derivable judgement $J$, we can give a derivation for $J$ by backward chaining.
\end{lemma}

\begin{proof}
    Backward chaining maintains a queue of goals, judgements whose derivations are to be sought. Initially this consists of the sole judgement we want to derive. At each step, we pick a goal, then we pick a rule whose conclusion is our picked goal and add the premises of the rule to our list of goals. Since $J$ is derivable there must be a derivation that can be chosen.
\end{proof}

\begin{remark}
    We could as before consider all possible goals generated by all possible rules which would technically give us an algorithm like in the case for forward chaining. But it would also be as useless as that algorithm. What backward chaining allows us to do however is better pick to rules at each stage. This is the structure that type checkers will take later on and even proof assistants, programs that assist a user in proving a statement formally. Due to each stage giving us information about the kind of rule we ought to pick, backward chaining is more suitable for algorithmic ally proving something. In face if we set up our rules in such a way that for each goal there is only one such rule to pick, we have an algorithm!
\end{remark}

\subsection{Rule induction}

Conveniently our notion of inductive definition of a judgement form is actually an inductive definition. In that the set of derivable judgements forms a well-founded tree as defined earlier. This means we can apply our more general notion of well-founded induction when proving properties of a judgement.

\begin{defin}
    We say that a property $\mathcal{P}$ is \emph{closed under} or \emph{respects} the rules defining a judgement form $\mathsf{J}$. A property $\mathcal{P}$ respects the rule
    $$
        \frac
        {a_1\ \mathsf{J}\quad \cdots \quad a_k\ \mathsf{J}}
        {a \ \mathsf{J}}
    $$
    if $\mathcal{P}(a)$ holds whenever $\mathcal{P}(a_1), \dots, \mathcal{P}(a_k)$ do.
\end{defin}

\begin{remark}
    This is nothing more than a rephrasing of well-founded trees which is classically more common. This style of inductive definition fits more closely with what is actually going on, and we would argue is easier to work with.
\end{remark}


We will now give some examples detailing how rule induction can be used.

\begin{example}
    Continuing our $\mathsf{nat}$ example, if we want to show $\mathcal{P}(a)$ for some $a\ \mathsf{nat}$ it is enough to show the following:
    \begin{itemize}
        \item $\mathcal{P}(\texttt{zero})$.
        \item for all $a$, of $\mathcal{P}(a)$, then $\mathcal{P}(\texttt{succ}(a))$.
    \end{itemize}
    
    This is the familiar notion of mathematical induction on the natural numbers.
\end{example}

Now for another example where we combine all the things we have just discussed.

\begin{example}
    Consider the judgement form $\mathsf{tree}$ defined inductively by the following rules:
    $$
        \frac
        {}
        {\texttt{empty}\ \mathsf{tree}}
        \qquad \qquad
        \frac
        {a_1 \ \mathsf{tree} \quad a_2 \ \mathsf{tree}}
        {\mathsf{node}(a_1;a_2)\ \mathsf{tree}}
    $$
    Here is a derivation of the judgement $\texttt{node}(\texttt{empty};\texttt{node}(\texttt{empty};\texttt{empty}))\ \mathsf{tree}$:
    \begin{prooftree}
        \AxiomC{}
        \UnaryInfC{$\texttt{empty}\ \mathsf{tree}$}
        \AxiomC{}
        \UnaryInfC{$\texttt{empty}\ \mathsf{tree}$}
        \AxiomC{}
        \UnaryInfC{$\texttt{empty}\ \mathsf{tree}$}
        \BinaryInfC{$\mathsf{node}(\texttt{empty};\texttt{empty})\ \mathsf{tree}$}
        \BinaryInfC{$\texttt{node}(\texttt{empty};\texttt{node}(\texttt{empty};\texttt{empty}))\ \mathsf{tree}$}
    \end{prooftree}
    Now rule induction for the judgement form $\mathsf{tree}$ states that, to show $\mathcal{P}(a)$ it is enough to show the following:
    \begin{itemize}
        \item $\mathcal{P}(\texttt{empty})$.
        \item for all $a_1$ and $a_2$, if both $\mathcal{P}(a_1)$ and $\mathcal{P}(a_2)$ then, $\mathcal{P}(\texttt{node}(a_1; a_2))$. 
    \end{itemize}
    This is the familiar notion of tree induction.
\end{example}

Now that we have induction on our inductive definitions we can prove some results about our examples.

\begin{lemma}
    If $\texttt{succ}(a)\ \mathsf{nat}$, then $a\ \mathsf{nat}$.
\end{lemma}

\begin{proof}
    By induction on $\texttt{succ}(a)$, when $\texttt{succ}(a)$ is $\texttt{zero}$ this is vacuously true. Otherwise when $\texttt{succ}(a)$ is $\text{succ}(b)$, what we want to prove is $\texttt{succ}(b)\ \mathsf{nat} \implies b\ \mathsf{nat}$ but this is exactly our induction hypothesis.
\end{proof}


\begin{lemma}[Reflexivity of $=$]
    If $a\ \mathsf{nat}$, then $a = a$.
\end{lemma}

\begin{proof}
    By induction on $a$ we have two cases which are exactly the two rules about $=$ to begin with.
\end{proof}


\begin{lemma}[Injectivity of \texttt{succ}]\label{inj_succ}
    If $\texttt{succ}(a_1) = \texttt{succ}(a_2)$, then $a_1 = a_2$.
\end{lemma}

\begin{proof}
    We perform induction on $\texttt{succ}(a_1)$ and $\texttt{succ}(a_2)$. Note that if any of the two are of the form $\texttt{zero}$ then the statement is true vacuously. When $\texttt{succ}(a_1)$ is of the form $\texttt{succ}(b_1)$ and $\texttt{succ}(a_2)$ is of the form $\texttt{succ}(b_2)$ our statement that we want to prove is exactly what we get from the induction hypothesis.
\end{proof}


\begin{lemma}[Symmetry of $=$]
    If $a = b$, then $b = a$.
\end{lemma}

\begin{proof}
    Begin with induction on $a$ and $b$:
    \begin{itemize}
        \item Suppose $a$ is of the form $\texttt{zero}$ and $b$ is of the form $\texttt{zero}$ then we have $\texttt{zero} = \texttt{zero}$ as desired.
        \item Suppose $a$ is of the form $\texttt{zero}$ and $b$ is of the form $\texttt{succ}(b')$ then our statement is vacuously true. The same happens for when $b$ is $\texttt{zero}$ and $a$ is of the form $\texttt{succ}(a')$.
        \item Finally when $a$ is of the form $\texttt{succ}(a')$ and $b$ is of the form $\texttt{succ}(b')$ we have $\texttt{succ}(a')= \texttt{succ}(b')$. By \ref{inj_succ} we have $a'=b'$ and by our induction hypothesis we have $b' = a'$ as desired.
    \end{itemize}
\end{proof}


\begin{lemma}[Transitivity of $=$]
    If $a = b$ and $b = c$ then $a = c$.
\end{lemma}

\begin{proof}
    By induction on $a$, $b$ and $c$ we see that we have eight cases. Clearly six of these are vacuously true, so we will prove the other two:
    \begin{itemize}
        \item When $a$, $b$ and $c$ are of the form $\texttt{zero}$ our statement holds trivially.
        \item When $a$, $b$ and $c$ are of the form $\texttt{succ}(a')$, $\texttt{succ}(b')$ and $\texttt{succ}(c')$ respectively, we can apply \ref{inj_succ} on $\texttt{succ}(a') =\texttt{succ}(b')$ and $\texttt{succ}(b') = \texttt{succ}(c')$ to get $a' = b'$ and $b' = c'$. Then applying our induction hypothesis we have $a' = c'$, finally applying the second rule for $=$ we have $\texttt{succ}(a') =\texttt{succ}(c')$.
    \end{itemize}
\end{proof}

Finally we can say our four rules correspond to Peano arithmetic!

\begin{remark}
    Classical presentations of Peano arithmetic contain many more axioms to formulate. We can already see how this inductive approach is cleaner and more compact than classical sporadic presentations \cite{GlossarWiki:Goedel:1931}.
\end{remark}

\begin{remark}
    The statements that we have proven about Peano arithmetic is technically considered a \emph{meta statement}, since the truth value of a judgement such as $a = b$ occurs in the logic we set everything up in. When we consider richer systems of judgements, we may be able to prove meta statements, internally.
\end{remark}

\subsection{Hypothetical judgements}

A \emph{hypothetical judgement} expresses an entailment between one or more hypothesis and a conclusion. There are two main notions of entailment in logic: \emph{derivability} and \emph{admissibility}. We first begin by defining derivability.

\begin{defin}
    Given a set $\mathcal{R}$ of rules, define the \emph{derivability} judgement, $J_1,\dots,J_k \vdash_{\mathcal{R}} K$ where each $J_i$ and $K$ are basic judgements, to mean that we may derive $K$ from the \emph{expansion} $\mathcal{R} \cup \{ J_1, \dots, J_k \}$ of the rules $\mathcal{R}$ with the axioms
    $$
        \frac{\qquad}{J_1} \quad \cdots \quad \frac{\qquad}{J_k}
    $$
    We treat the \emph{hypotheses} $J_1, \dots, J_k$ of the judgement $J_1,\dots,J_k \vdash_{\mathcal{R}} K$ as axioms and derive the \emph{conclusion}, by composing rules in $\mathcal{R}$. Thus $J_1,\dots,J_k \vdash_{\mathcal{R}} K$ means the judgement $K$ is derivable from the expanded rules $\mathcal{R} \cup \{ J_1, \dots, J_k \}$.
\end{defin}

\begin{remark}
    We will typically denote a list of basic judgements by a capital Greek letter such as $\Gamma$ or $\Delta$. The expansion $\mathcal{R} \cup \{ J_1, \dots, J_k \}$ may also be written as $\mathcal{R} \cup \Gamma$ where $\Gamma := J_1, dots, J_k$. The judgement $\Gamma \vdash_{\mathcal{R}} K$ means $K$ is derivable from the rules $\mathcal{R} \cup \Gamma$, and the judgement $\vdash _{\mathcal{R}} \Gamma$ means that $\vdash _{\mathcal{R}} J$ for each $J$ in $\Gamma$. We may also extend lists of basic judgements like this: $\Gamma, J$, which would correspond to the list of basic judgements $J_1, \dots, J_k, J$, similarly for $J, \Gamma$. We can then concatenate two lists of basic judgements in the obvious way, through list concatenation written $\Gamma, \Delta$.
\end{remark}

\begin{remark}
    Alternative names for \emph{hypothesis} and \emph{conclusion} include \emph{antecedent} and \emph{consequent} respectively.
\end{remark}

\begin{example}
    Let $\mathsf{Peano}$ be the set of four rules for our $\mathsf{nat}$ example. Consider the following derivability judgement:
    $$a\ \mathsf{nat} \vdash_{\mathsf{Peano}} \texttt{succ}(\texttt{succ}(a))\ \mathsf{nat}$$
    This can be shown to be true by exhibiting the following derivation:
    \begin{prooftree}
        \AxiomC{$a\ \mathsf{nat}$}
        \UnaryInfC{$\texttt{succ}(a)\ \mathsf{nat}$}
        \UnaryInfC{$\texttt{succ}(\texttt{succ}(a))\ \mathsf{nat}$}
    \end{prooftree}
\end{example}

We now show that derivability doesn't get affected by expansion.

\begin{lemma}[Stability]
    If $\Gamma \vdash_{\mathcal{R}} J$, then $\Gamma \vdash_{\mathcal{R} \cup \mathcal{R'}} J$.
\end{lemma}

\begin{proof}
    Any derivation if $J$ from $\mathcal{R} \cup \Gamma$ is also a derivation from $(\mathcal{R} \cup \mathcal{R}') \cup \Gamma$ since $\mathcal{R} \subseteq \mathcal{R}\cup \mathcal{R}'$.
\end{proof}

There are a number of structural properties that derivability satisfies:

\begin{lemma}[Reflexivity]
    Every judgement is a consequence of itself: $$\Gamma, J\vdash_{\mathcal{R}} J$$
\end{lemma}

\begin{proof}
    Since $J$ becomes an axiom, the proof is trivial.
\end{proof}


\begin{lemma}[Weakening]
    If $\Gamma \vdash_{\mathcal{R}} J$, then $\Gamma, K \vdash_{\mathcal{R}} J$. Entailment is not influenced by unused premises.
\end{lemma}

\begin{proof}
    The proof is trivial.
\end{proof}


\begin{lemma}[Transitivity]\label{derivability_transitivity}
    If $\Gamma, K \vdash_{\mathcal{R}} J$ and $\Gamma \vdash_{\mathcal{R}} K$, then $\Gamma \vdash_{\mathcal{R}} J$. If we replace an axiom by a derivation of it, the result is a derivation of the consequent without the hypothesis.
\end{lemma}

\begin{proof}
    It is clear that if there is a derivation for $J$ from $\Gamma,K \cup \mathcal{R}$ and a derivation for $K$ from $\Gamma \cup \mathcal{R}$, then there is clearly a derivation for $J$ from $\Gamma \cup \mathcal{R}$. For the first case it is clear how to compose two derivations to give the desired derivation.
\end{proof}

\begin{defin}
    Another form of entailment, \emph{admissibility}, written $\Gamma \vDash_{\mathcal{R}} J$, is a weaker form of hypothetical judgement stating that $\vdash_{\mathcal{R}} \Gamma$ implies $\vdash_{\mathcal{R}} J$. That is, the conclusion $J$ is derivable from the rules $\mathcal{R}$ when the assumptions are all derivable from the rules $\mathcal{R}$.
\end{defin}

\begin{remark}
    In particular, if any of the hypotheses are \emph{not} derivable relative to $\mathcal{R}$, then the judgement is \emph{vacuously} true.
\end{remark}


The admissibility judgement is \emph{not} stable under expansion of the rules.

\begin{lemma}
    If $\Gamma \vdash_{\mathcal{R}} J$, then $\Gamma \vDash_{\mathcal{R}} J$.
\end{lemma}

\begin{proof}
    By definition of admissibility we need to show that $\vdash_{\mathcal{R}} \Gamma$ implies $\vdash_{\mathcal{R}} J$. It can be seen that repeated application of transitivity allows us to form a similar statement for when $K$ is a list of basic judgements in reference to \ref{derivability_transitivity}. This repeated transitivity gives us the desired result.
\end{proof}

We will now give an example of some inadmissible rules.

\begin{example}
    Consider the collection of rules $\mathsf{Parity}$ consisting of the rules in $\mathsf{Peano}$ and the following:
    $$
        \frac{}{\texttt{zero}\ \mathsf{even}} \qquad
        \frac{b\ \mathsf{odd}}{\texttt{succ}(b)\ \mathsf{even}} \qquad
        \frac{a\ \mathsf{even}}{\texttt{succ}(a)\ \mathsf{odd}}
    $$
    This is a simultaneous inductive definition. Clearly we have the following admissibility judgement
    $$ \texttt{succ}(a)\ \mathsf{even} \vDash_{\mathsf{Parity}} a\ \mathsf{odd} $$
    But by adding the following rule to $\mathsf{Parity}$, and calling it $\mathsf{Parity}'$
    $$
        \frac{}{\texttt{succ}(\texttt{zero}) \ \mathsf{even}}
    $$
    we see that the following is no longer true:
    $$ \texttt{succ}(a)\ \mathsf{even} \vDash_{\mathsf{Parity'}} a\ \mathsf{odd} $$
    since there is no composition of rules deriving $\texttt{zero}\ \mathsf{odd}$.
    Hence admissibility is not stable under expansion.
\end{example}

\begin{remark}
    Admissibility is a useful property of a rule. It essentially checks whether we can get rid of a rules, knowing that we can derive it anyway. Hence by identifying inadmissible rules we can streamline our rule set.
\end{remark}

\subsection{Hypothetical inductive definitions}

Our inductive definitions give us a rich and expressive way to define and use rules. We wish to enrich it further by introducing rules whose premises and conclusions are derivability judgements.

\begin{defin}
    A \emph{hypothetical inductive definition} consists of a set of \emph{hypothetical rules} of the following form:
    $$
        \frac{\Gamma, \Gamma_1 \vdash J_1 \quad \cdots \quad \Gamma, \Gamma_n \vdash J_n}{\Gamma \vdash J}
    $$
    We call the hypotheses $\Gamma$, the \emph{global hypotheses} of the rule, and $\Gamma_i$ are called the local hypotheses of the $i$th premise of the rule. We will require that all rules in a hypothetical inductive definition be \emph{uniform} in the following sense.
\end{defin}

\begin{defin}
    A hypothetical rules is said to be \emph{uniform} if it holds for \emph{all} global contexts.
\end{defin}

\begin{remark}
    When we have uniformity, we can present the rule in an \emph{implicit} or \emph{local} form:
    $$
        \frac{\Gamma_1 \vdash J_1 \quad \cdots \quad \Gamma_n \vdash J_n}{J}
    $$
    with the understanding that the rule applies for any choice of global hypotheses.
\end{remark}

\begin{remark}
    A hypothetical inductive definition can be regarded as an ordinary inductive definition of a \emph{formal derivability judgement} $\Gamma \vdash J$ consisting of a list of basic judgements $\Gamma$ and a basic judgement $J$.
\end{remark}

\begin{defin}\label{form_deri}
    A \emph{formal derivability judgement} $\Gamma \vdash J$ is closed under a set of hypothetical rules $\mathcal{R}$ and the judgement is \emph{structural} is that it is closed under the following rules
    $$
        \frac{}{\Gamma, J \vdash J} \qquad
        \frac{\Gamma \vdash J}{\Gamma , K \vdash J} \qquad
        \frac{\Gamma \vdash K \quad \Gamma , K \vdash J}{\Gamma \vdash J}
    $$
    These rules ensure that formal derivability behaves like a hypothetical judgement. We write $\Gamma \vdash_{\mathcal{R}} J$ to denote that $\Gamma \vdash J$ is derivable from rules $\mathcal{R}$.
\end{defin}

\begin{remark}
    This definition is perhaps quite confusing, this is because we have two layers of derivability. What a formal derivability judgement shows is that the judgement of being derivable is itself derivable. This also means that we do not have to define what hypothetical induction on a hypothetical inductive definition is, since the formal derivability judgement is itself a judgement. So the principle of \emph{hypothetical rule induction} is just the principle of rule induction applied to the formal hypothetical rule induction.
\end{remark}

\begin{remark}\label{structural_remark}
    In the context of type theory, basic judgements are typically assertions that a term $a$ has a type $A$, written $a : A$. In this case $\Gamma$ becomes known as a \emph{typing context}. In this case, the first structural rule in Definition \ref{form_deri} is typically known as the \emph{variable}.
    The second rule is known as \emph{weakening}, this states that ``having knowledge of an extra variable does not effect what you know''. Finally the third would correspond to ``being able to forget an assumption, if you know you can derive it''. In the case of the simply typed lambda calculus, which will study later, we will see that such structural rules are admissible. 
\end{remark}

The idea of logical judgements builds on the work of Per Martin-L\"of \cite{martin1996meanings, martin1984intuitionistic}.

