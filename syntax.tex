% harper

\section{Syntax}

\subsection{Introduction}

We will follow the structure of syntax outlined in Harper \cite{harper_2016}. There are several reasons for this. 

Firstly, for example in Barendregt et. al. \cite{BarendregtHenk2013Lcwt} we have notions of substitution left to the reader under the assumption that they can be fixed. Generally Barendregt's style is like this and even when there is much formalism, it is done in a way that we find peculiar.

In Crole's book \cite{CroleRoyL1993Cft}, syntax is derived from an \textit{algebraic signature} which comes directly from categorical semantics. We want to give an independent view of type theory. The syntax only has types as well, meaning that only terms can be posed in this syntax. Operations on types themselves would have to be handled separately. This will also make it difficult to work with \textit{bound variables}.

In Lambek and Scott's book \cite{LambekJ1986Itho}, very little attention is given to syntax and categorical semantics and deriving type theory from categories for study is in the forefront of their focus.

In Jacob's book \cite{JacobsCLTT}, we again have much reliance on categorical machinery. A variant of algebraic signature called a \mathit{many-typed signature} is given, which has its roots in mathematical logic. Here it is discussed that classically in logic the idea of a sort and a type were synonymous, and they go onto preferring to call them types. This still has the problems identified before as terms and types being treated separately, when it comes to syntax.

In Barendregt's older book \cite{barendregt1984lambda}, there are models of the syntax of (untyped) lambda calculus, using Scott topologies on complete lattices. We acknowledge that this is a working model of the lambda calculus but we believe it to be overly complex for the task at hand. It introduces a lot of mostly irrelevant mathematics for studying the lambda calculus. And we doubt very much that these models will hold up to much modification of the calculus. Typing seems impossible.

In S{\o}rensen and Urzyczyn's book \cite{Sorensen:2006:LCI:1197021} a more classical unstructured approach to syntax is taken. This is very similar to the approaches that Church, Curry and de Brujin gave early on. The difficulty with this approach is that it is very hard to prove things about the syntax. There are many exceptional cases to be weary of (for example if a variable is bound etc.). It can also mean that the syntax is vulnerable to mistakes. We acknowledge it's correctness in this case, however we prefer to use a safer approach.

We will finally look at one more point of view, that of mathematical logic. We look at Troelstra and Schwichtenberg's book \cite{troelstra_schwichtenberg_2000} which studies proof theory. This is essentially the previous style but done to a greater extent, for they use that kind of handling of syntax to argue about more general logics. As before, we do not choose this approach.

We have seen books from either end of the spectrum, on one hand Barendregt's type theoretic camp, and on the other, the more categorical logically oriented camp. We have argued that the categorical logically oriented texts do not do a good job of explaining and defining syntax, their only interest is in their categories. The type theoretic texts also seem to be on mathematically shaky ground, sometimes much is left to the reader and finer details are overlooked.

Harper's seems more sturdy and correct in our opinion. Harper doesn't concern himself with abstraction for the sake of abstraction but rather when it will benefit the way of thinking about something. The framework for working with syntax also seems ideal to work with, when it comes to adding features to a theory (be it a type theory or otherwise).

\subsection{Well-founded induction}

Firstly we will begin a quick recap of induction. This should be a notion familiar to computer scientists and mathematicians alike. The following will be more accessible to mathematicians but probably more useful for them too since they will be generally less familiar with the generality of induction.


The notion of well-founded induction is a standard theorem of set theory. The classical proof of which usually uses the law of excluded middle \cite[p. 62]{johnstone1987notes}, \cite[Ch. 7]{barwise1982handbook}. It's use in the formal semantics of programming languages is not much different either \cite[Ch. 3]{winskel1993formal}. There are however more constructive notions of well-foundedness \cite[\S 8]{2018arXiv180805204S} with more careful use of excluded middle. We will follow \cite{10.2307/2275781}, as this is the simplest to understand, and we won't be using this material much other than an initial justification for induction in classical mathematics.

\begin{defin}
    Let $X$ be a set and $\prec$ a binary relation on $X$. A subset $Y \subseteq X$ is called \textbf{$\prec$-inductive} if
    $$
        \forall x \in X, \quad (\forall y \prec x,\ y \in Y) \Rightarrow x \in Y.
    $$
\end{defin}

\begin{defin}\label{wf}
    The relation $\prec$ is \textbf{well-founded} if the only $\prec$-inductive subset of $X$ is $X$ itself. A set $X$ equipped with a well-founded relation is called a \textit{well-founded set}.
\end{defin}

\begin{theorem}[Well-founded induction principle]
    Let $X$ be a well-founded set and $P$ a property of the elements of $X$ (a proposition). Then
    $$
        \forall x \in X, P(x) \Leftrightarrow \forall x \in X,\quad (\forall y \prec x, P(y)) \Rightarrow P(x).
    $$
\end{theorem}
\begin{proof}
    The forward direction is clearly true. For the converse, assume $\forall x \in X,((\forall y \prec x, P(y)) \Rightarrow P(x))$. Note that $P(y) \Leftrightarrow x \in Y := \{ x \in X \mid P(x)\} $ which means our assumption is equivalent to $\forall x \in X,\ (\forall y \prec x,\ y \in Y) \Rightarrow x \in Y$ which means $Y$ is $\prec$-inductive by definition. Hence by \ref{wf} $Y=X$ giving us $ \forall x \in X, P(x)$.
\end{proof}

We now get onto some of the tools we will be using to model the syntax of our type theory. 

\subsection{Abstract syntax trees}

We begin by outlining what exactly syntax is, and how to work with it. This will be important later on if we want to prove things about our syntax as we will essentially have good data structures to work with.

%We will begin with the notion of an {\it abstract syntax tree}. Which can be what is informally known as syntax, thus formal statements about the syntax are referring to its manifestation as an abstract syntax tree.

%% Sort
\begin{defin}[Sorts]
    Let $\mathcal{S}$ be a finite set, which we will call \textbf{sorts}. An element of $\mathcal{S}$ is called a \textbf {sort}.
\end{defin}

A sort could be a term, a type, a kind or even an expression. It should be thought of an abstract notion of the kind of syntactic element we have. Examples will follow making this clear.

%% Arity
\begin{defin}[Arities]
    An \textbf{arity} is an element $((s_1,\dots,s_n),s)$ of the set of \textbf{arities} $\mathcal{Q}:=\mathcal{S}^\star \times \mathcal{S}$ where $\mathcal{S}^\star$ is the Kleene-star operation on the set $\mathcal{S}$ (a.k.a the free monoid on $\mathcal{S}$ or set of finite tuples of elements of $\mathcal{S}$). An arity is typically written as $(s_1,\dots,s_n)s$. 
\end{defin}

%% Operator
\begin{defin}[Operators]
    Let $\mathcal{O} :=\{ \mathcal{O}_\alpha \}_{\alpha \in \mathcal{Q}}$ be an $\mathcal{Q}$-indexed (arity-indexed) family of disjoint sets of \textbf{operators} for each arity. An element $o \in \mathcal{O}_\alpha$ is called an \textbf{operator} of arity $\alpha$. If $o$ is an operator of arity $(s_1,\dots,s_n)s$ then we say $o$ has \textbf{sort} $s$ and that $o$ has $n$ \textbf{arguments} of sorts $s_1,\dots,s_n$ respectively.
\end{defin}

%% Variables
\begin{defin}[Variables]
    Let $\mathcal{X}:= \{ \mathcal{X}_s\}_{s \in \mathcal{S}}$ be an $\mathcal{S}$-indexed (sort-indexed) family of disjoint (finite?) sets $\mathcal{X}_s$ of \textbf{variables} of sort $s$. An element $x\in\mathcal{X}_s$ is called a \textbf{variable} $x$ of \textbf{sort} $s$. 
\end{defin}

%% Fresh variables
\begin{defin}[Fresh variables]
    We say that $x$ is \textbf{fresh} for $\mathcal{X}$ if $x \not\in \mathcal{X}_s$ for any sort $s\in \mathcal{S}$. Given an $x$ and a sort $s\in \mathcal{S}$ we can form the family $\mathcal{X},x$ of variables by adding $x$ to $\mathcal{X}_s$. 
\end{defin}


[[ Wording here may be confusing]]
%% Fresh variable sets
\begin{defin}[Fresh sets of variables]
    Let $V=\{ v_1 ,\dots, v_n\}$ be a finite set of variables (which all have sorts implicitly assigned so really a family of variables $\{V_s\}_{s\in\mathcal{S}}$ indexed by sorts, where each $V_s$ is finite). We say $V$ is fresh for $\mathcal{X}$ by induction on $V$. Suppose $V=\varnothing$, then $V$ is \mathbf{fresh} for $X$. Suppose $V = \{v \} \cup W$ where $W$ is a finite set, $v$ is fresh for $W$ and $W$ is fresh for $\mathcal{X}$. Then $V$ is fresh for $\mathcal{X}$ if $v$ is fresh for $\mathcal{X}$. By induction we have defined a finite set being fresh for a set $\mathcal{X}$. Write $\mathcal{X},V$ for the union (which is disjoint) of $\mathcal{X}$ and $V$. This gives us a new set of variables with obvious indexing.
\end{defin}

%% Remark about notation for adding variables
\begin{remark}
    The notation $\mathcal{X},x$ is ambiguous because the sort $s$ associated to $x$ is not written. But this can be remedied by being clear from the context what the sort of $x$ should be.
\end{remark}

%% Abstract syntax trees
\begin{defin}[Abstract syntax trees]
    The family $\mathcal{A}[\mathcal{X}]=\{ \mathcal{A}[\mathcal{X}]_s \}_{s \in \mathcal{S}}$ of \textbf{abstract syntax trees} (or asts), of \textbf{sort} $s$, is the smallest family satisfying the following properties:
    
    \begin{enumerate}
        \item A variable $x$ of sort $s$ is an ast of sort $s$: if $x \in \mathcal{X}_s$, then $x \in \mathcal{A}[\mathcal{X}]_s$.
        
        \item Operators combine asts: If $o$ is an operator of arity $(s_1, \dots, s_n)s$, and if $a_1 \in \mathcal{A}[\mathcal{X}]_{s_1}, \dots, a_n \in \mathcal{A}[\mathcal{X}]_{s_n}$, then $o(a_1;\dots; a_n) \in \mathcal{A}[\mathcal{X}]_s$.
    \end{enumerate}
\end{defin}

%% Remark about inductively generated sets
\begin{remark}
    The idea of a smallest family satisfying certain properties is that of structural induction. So another way to say this would be a family of sets inductively generated by the following constructors.
\end{remark}

%% Remark about asts being trees
\begin{remark}
    An ast can be thought of as a tree whose leaf nodes are variables and branch nodes are operators. 
\end{remark}

%% Lambda calculus example
\begin{example}[Syntax of lambda calculus]
    The (untyped) lambda calculus has one sort $\Term$, so $\mathcal{S} = \{ \Term \} $. We have an operator $\App$ of application whose arity is $(\Term, \Term)\Term$ and an family of operators $\{\lambda_x \}_{x \in \Var}$ which is the lambda abstraction with bound variable $x$, so $\mathcal{O} = \{ \lambda_x \} \cup \{ \App \} $. The arity of each $\lambda_x$ for some $x \in \Var$ is simply $(\Term) \Term$.
    
    Consider the term $$\lambda x . (\lambda y . x y)  z$$

    We can consider this the \textit{sugared} version of our syntax. If we were to \textit{desugar} our term to write it as an ast it would look like this:

    $$
        \lambda_x(\App(\lambda_y(\App(x ; y)); z) )
    $$

    Sugaring allows for long-winded terms to be written more succinctly and clearly. Most readers would agree that the former is easier to read. We have mentioned the tree structure of asts so we will illustrate with the following equivalent examples. We present two to allow for use of both styles.
    
    \input{fig1.tex}
    
\end{example}

%% Remark about asts not caring about binding
\begin{remark}
    Note that later we will enrich our notion of abstract syntax tree that takes into account binding and scope of variables but for now this is purely structural.
\end{remark}

%% Remark about proving things by structural induction on asts
\begin{remark}
    When we prove properties $\mathcal{P}(a)$ of an ast $a$ we can do so by structural induction on the cases above. We will define structural induction as a special case of well-founded induction. But for this we will need to define a relation on asts.
\end{remark}

\begin{defin}
    Suppose $\mathcal{X} \subseteq \mathcal{Y}$. An ast $a \in \mathcal{A}[\mathcal{X}]$ is a \textbf{subtree} of an ast $b \in \mathcal{A}[\mathcal{Y}]$ [This part is giving me a headache. How can I define subtree if I can't do it by induction? To do it by induction I would have to define subtree.]
\end{defin}



[Some more notes on structural induction, perhaps this can be defined and discussed with trees in the section before?]

%% Plenty of examples of asts with examples of sorts, operators and variables
[add examples of sorts, operators, variables and how they fit together in asts]

%% Lemma about subsets of variables
\begin{lemma}
    If we have $\mathcal{X} \subseteq \mathcal{Y}$ then, $\mathcal{A}[\mathcal{X}] \subseteq \mathcal{A}[\mathcal{Y}]$.
\end{lemma}
\begin{proof}
    Suppose $\mathcal{X} \subseteq \mathcal{Y}$ and $a \in \mathcal{A}[\mathcal{X}]$, now by structural induction on $a$:
    
    \begin{enumerate}
        \item If $a$ is in $\mathcal{X}$ then it is obviously also in $\mathcal{Y}$.
        \item If $a := o(a_1;\dots;a_n) \in \mathcal{A}[\mathcal{X}]$ we have $a_1, \dots, a_n\in \mathcal{A}[\mathcal{X}]$ also. By induction we can assume these to be in $\mathcal{A}[\mathcal{Y}]$ hence giving us $a \in \mathcal{A}[\mathcal{Y}]$.
    \end{enumerate}
    
    Hence by induction we have shown that $\mathcal{A}[\mathcal{X}] \subseteq \mathcal{A}[\mathcal{Y}]$.
\end{proof}

\subsection{Substitution in asts}

%% Substitution
\begin{defin}[Substitution]\label{sub}
    If $a \in \mathcal{A}[\mathcal{X},x]_{s'}$, and $b \in \mathcal{A}[\mathcal{X}]_s$, then $[b/x]a \in \mathcal{A}[\mathcal{X}]_{s'}$ is the result of \textbf{substituting} $b$ for every occurrence of $x$ in $a$. The ast $a$ is called the \textbf{target}, the variable $x$ is called the \textbf{subject} of the \textbf{substitution}. We define substitution on an ast $a$ by induction:
    \begin{enumerate}
        \item $[b/x]x = b$ and $[b/x]y = y$ if $x\ne y$.
        \item $[b/x]o(a_1;\dots;a_n)=o([b/x]a_1;\dots;[b/x]a_n)$
    \end{enumerate}
\end{defin}

%% Examples of substitution
[Examples of substitution]

\begin{cor}\label{subcheck}
    If $a \in \mathcal{A}[\mathcal{X},x]$, then for every $b \in \mathcal{A}[\mathcal{X}]$ there exists a unique $c \in \mathcal{A}[\mathcal{X}]$ such that $[b/x]a = c$.
\end{cor}
\begin{proof}
    By structural induction on $a$, we have three cases: $a := x$, $a:=y$ where $y \ne x$ and $a := o(a_1; \dots; a_n)$. In the first we have $[b/x]x=b=c$ by definition. In the second we have $[b/x]y=y=c$ by definition. In both cases $c \in \mathcal{A}[\mathcal{X}]$ and are uniquely determined. Finally, when $a := o(a_1; \dots; a_n)$, we have by induction unique $c_1,\dots, c_n$ such that $c_i:=[b/x]a_i$ for $1 \le i \le n$. Hence we have a unique $c=o(c_1,\dots,c_n) \in \mathcal{A}[\mathcal{X}]$.
\end{proof}

\begin{remark}
    Note that \ref{subcheck} was simply about checking Definition \ref{sub}. We have written out a use of the definition here so we won't have to again in the future.
\end{remark}

Abstract syntax trees are our starting point for a well-defined notion of syntax. We will modify this notion, as the author of \cite{harper_2016} does, with slight modifications that are used in \cite{nlab:initiality_project, nlab:initiality_project_-_raw_syntax}, the Initiality Project. This is a collaborative project for showing initiality of dependent type theory (the idea that some categorical model is initial in the category of such models). It is a useful reference because it has brought many mathematicians together to discuss the intricate details of type theory. The definitions here have spawned from these discussions on the nlab and the nforum.

We want to modify the notion of abstract syntax tree to include features such as binding and scoping. This is a feature used by many type theories (and even the lambda calculus). It is usually added on later by keeping track of bound and free variables. [CITE]. We will avoid this approach as it makes inducting over syntax more difficult.

\subsection{Abstract binding trees}

%% Generalize arity
\begin{defin}[Generalized arities]
    A \textbf{generalised arity} (or signature) is a tuple consisting of the following data:
    
    \begin{enumerate}
        \itemsep -0.5 \parsep
        \item A sort $s \in \mathcal{S}$.
        
        \item A list of sorts of length $n$ called the \textbf{argument sorts}, where $n$ is called the \textbf{argument arity}.
        
        \item A list of sorts of length $m$ called the \textbf{binding sorts}, where $m$ is called the \textbf{binding arity}.
        
%        \item A natural number $n \in \N$ called the \textbf{argument arity}.
%        \item A function $\mathrm{soa} : [n] \to \mathcal{S}$ choosing the sorts of the $n$ arguments. Where $\mathrm{soa}$ means \textbf{sort of argument}.
%        \item A natural number $m \in \N$ called the \textbf{binding arity} (or number of bound variables).
%        \item A function $\mathrm{sov} : [m] \to \mathcal{S}$ choosing the sorts of the $m$ bound variables. Where $\mathrm{sov}$ means \textbf{sort of variable}.
        \item A decidable relation $\scope$ between $[n]$ and $[m]$ called \textbf{scoping}. Where $j \scope k$ means the $j$th argument is in scope of the $k$th bound variable.
    \end{enumerate}
    
    The set of generalised arities $\mathbf{GA}$ could therefore be defined as $\mathcal{S} \times \mathcal{S}^\star  \times \mathcal{S}^\star$ equipped with some appropriate relation $\scope$.
\end{defin}

\begin{remark}
    In \cite{harper_2016} there is no relation but a function. And each argument has bound variables assigned to it. But as argued in \cite{nlab:initiality_project_-_raw_syntax} this means arguments can have different variables bound even if they are really the same variable. To fix this, bound variables belong to the whole signature. Which confidently makes it simpler to understand too.
    
    This definition is more general than the definition given in \cite{nlab:initiality_project_-_raw_syntax} due to bound variables having sorts chosen for them rather than being defaulted to the sort $\mathrm{tm}$. It is mentioned there however that it can be generalised to this form (but would have little utility there).
\end{remark}

We will now redefine the notion of operator, taking note that generalised arities are a super-set of arities defined previously.

%% Operator
\begin{defin}[Operators (with generalized arity)]
    Let $\mathcal{O}:=\{ \mathcal{O}_\alpha\}_{\alpha \in \mathbf{GA}}$ be a $\mathbf{GA}$-indexed family of disjoint sets of \textbf{operators} for each generalised arity $\alpha$. An element $o \in \mathcal{O}_{\alpha \in \mathbf{GA}}$ is called an \mathbf{operator} of (generalised) \textbf{arity} $\alpha$. If $\alpha$ has sort $s$ then $o$ has \textbf{sort} $s$. If $\alpha$ has argument sorts $(s_1,\dots,s_n)$ then we say that $o$ has \textbf{argument arity} $n$, with the $j$th argument having \textbf{sort} $s_j$. If $\alpha$ has binding sorts $(t_1,\dots,t_m)$ then we say that $o$ has \textbf{binding arity} $m$, with the $k$th bound variable having \textbf{sort} $s_k$. If the the scoping relation of $\alpha$ has $j \scope k$ then we say that the $j$th argument of $o$ is in \textbf{scope} of the $k$th bound variable of $o$. 
\end{defin}

\begin{remark}
    We overload the definitions of arity and operator to mean generalised operator and operator with generalised arity respectively.
\end{remark}

Now that we can equip our operators with the datum of binding and scoping we can go ahead and define abstract binding trees.

[[ Lots of concepts for asts have been redefined for abts,
perhaps its worth making note of that back in the asts definitions ]]

%% Abstract binding trees
\begin{defin}[Abstract binding trees]\label{abt}
    The family $\mathcal{B}[\mathcal{X}] = \{ \mathcal{B}[\mathcal{X}]_s \}_{s \in \mathcal{S}}$ of \textbf{abstract binding trees} (or abts), of \textbf{sort} $s$, is the smallest family satisfying the following properties:
    
    \begin{enumerate}
        \item A variable $x$ of sort $s$ is an abt of sort $s$: if $x \in \mathcal{X}_s$, then $x \in \mathcal{B}[\mathcal{X}]_s$.
        \item Suppose $\mathtt{G}$ is an operator of sort $s$, argument arity $n$ and binding arity $m$. Suppose $V$ is some finite set of length $m$ which is fresh for $\mathcal{X}$. These will be called our \textbf{bound variables}. Label the elements of $V$ as $V= \{ v_1, \dots, v_m\}$. For $j\in [n]$, let $X_j:=\{ v_k \in V \mid j \scope k\}$ be the set of bound variables that the $j$th argument is in scope of. Now suppose for each $j \in [n]$, $M_j \in \mathcal{B}[\mathcal{X},V]_{s_j}$ where $s_j$ is the sort of the $j$th argument of $\mathtt{G}$. Then $\mathtt{G}(X;M_1,\dots, M_n) \in \mathcal{B}[\mathcal{X}]_s$.
    \end{enumerate}
\end{defin}

\begin{remark}
    There is a lot going on in the second constructor of Definition \ref{abt}. It simply allows for bound variables to be constructed in syntax in a well-defined way that avoids variable capture. This will be useful when defining notions like substitution on abts as we will have the avoidance of variable capture built-in.
\end{remark}

[[What is variable capture talk about this and reference this stuff because lots of cleverer people have thought about this too you know.]]
\subsection{Substitution in abts}

